{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2\n",
    "from sklearn import svm\n",
    "import numpy as np\n",
    "import glob\n",
    "import functools\n",
    "from skimage.feature import local_binary_pattern\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "import timeit\n",
    "import pickle\n",
    "from skimage.feature import greycomatrix, greycoprops\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Helper Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "takes a function and it's arguments and then call it and calculate the time it takes to run\n",
    "and return it with the returned value\n",
    "'''\n",
    "def function_timer(func, arg1, arg2, arg3):\n",
    "    s = timeit.default_timer()\n",
    "    val = func(arg1, arg2, arg3)\n",
    "    f = timeit.default_timer()\n",
    "    return (round((f-s)*100)/100.0) , val\n",
    "'''\n",
    "ceates a file with the given name and returnthe file object\n",
    "if the file already exist it opens it and wipes it out\n",
    "'''\n",
    "def open_file(fileName):\n",
    "    return open(fileName, \"w\")  #a --> for append       w --> for overwrite\n",
    "'''\n",
    "write a specific number in a given file object\n",
    "'''\n",
    "def write_in_file(file,number):\n",
    "    file.write(str(number)+\"\\n\")\n",
    "    return\n",
    "'''\n",
    "closes a given file object\n",
    "'''\n",
    "def close_file(file):\n",
    "    file.close()\n",
    "    return"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Read data Pathes of Males and Females"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "Male_training_data='Dataset/Males/Males/*.jpg'\n",
    "Female_training_data='Dataset/Females/Females/*.jpg'\n",
    "'''\n",
    "Male_training_data='Data_split/train/Males/*.jpg'\n",
    "Female_training_data='Data_split/train/Females/*.jpg'\n",
    "'''\n",
    "Male_testing_data='Data_split/test/Males/*.jpg'\n",
    "Female_testing_data='Data_split/test/Females/*.jpg'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Preprocessing  Module"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "def Preprocessing (image) : \n",
    "    image = cv2.cvtColor(image,cv2.COLOR_RGB2GRAY)  # convert color from RGB to GRAY\n",
    "    height, width = image.shape # get image dimensions\n",
    "    img = cv2.GaussianBlur(image, (9, 9), 0) #decrease noise for dialation\n",
    "    img = cv2.adaptiveThreshold(img, 255, cv2.ADAPTIVE_THRESH_GAUSSIAN_C,cv2.THRESH_BINARY_INV, 101, 30) # apply threshold on blured image\n",
    "    image = cv2.adaptiveThreshold(image, 255, cv2.ADAPTIVE_THRESH_GAUSSIAN_C,cv2.THRESH_BINARY_INV, 101, 30)  # apply threshold on original image\n",
    "    kernel = cv2.getStructuringElement(cv2.MORPH_RECT, (15, 20)) \n",
    "    img = cv2.dilate(img, kernel, iterations=8)\n",
    "    contours = cv2.findContours(img, cv2.RETR_EXTERNAL, cv2.CHAIN_APPROX_SIMPLE)[0] \n",
    "    biggest_contour = functools.reduce(lambda c1, c2: c1 if cv2.contourArea(c1) > cv2.contourArea(c2) else c2,contours) #find the biggest contour for text area\n",
    "    x, y, w, h = cv2.boundingRect(biggest_contour) # find smallest rect that can contain the text area after dialation\n",
    "    image = image[y:y + h, x:x + w]\n",
    "    return image"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Feature Extraction Module"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1- COLD features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "#define some constants for cold feature extraction \n",
    "N_RHO_BINS = 7\n",
    "N_ANGLE_BINS = 12\n",
    "N_BINS = N_RHO_BINS * N_ANGLE_BINS\n",
    "BIN_SIZE = 360 // N_ANGLE_BINS\n",
    "R_INNER = 5.0\n",
    "R_OUTER = 35.0\n",
    "K_S = np.arange(3, 8)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_contour_pixels(bw_image):\n",
    "        contours, _= cv2.findContours(bw_image, cv2.RETR_TREE, cv2.CHAIN_APPROX_NONE) \n",
    "        # contours = imutils.grab_contours(contours)\n",
    "        contours = sorted(contours, key=cv2.contourArea, reverse=True)[1:]\n",
    "        \n",
    "        img2 = bw_image.copy()[:,:,np.newaxis]\n",
    "        img2 = np.concatenate([img2, img2, img2], axis = 2)\n",
    "        return contours"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_cold_features(bw_image, approx_poly_factor = 0.01):\n",
    "    \n",
    "        contours = get_contour_pixels(bw_image)\n",
    "        \n",
    "        rho_bins_edges = np.log10(np.linspace(R_INNER, R_OUTER, N_RHO_BINS))\n",
    "        feature_vectors = np.zeros((len(K_S), N_BINS))\n",
    "        \n",
    "        # print([len(cnt) for cnt in contours])\n",
    "        for j, k in enumerate(K_S):\n",
    "            hist = np.zeros((N_RHO_BINS, N_ANGLE_BINS))\n",
    "            for cnt in contours:\n",
    "                epsilon = approx_poly_factor * cv2.arcLength(cnt,True)\n",
    "                cnt = cv2.approxPolyDP(cnt,epsilon,True)\n",
    "                n_pixels = len(cnt)\n",
    "                \n",
    "                point_1s = np.array([point[0] for point in cnt])\n",
    "                x1s, y1s = point_1s[:, 0], point_1s[:, 1]\n",
    "                point_2s = np.array([cnt[(i + k) % n_pixels][0] for i in range(n_pixels)])\n",
    "                x2s, y2s = point_2s[:, 0], point_2s[:, 1]\n",
    "                \n",
    "                thetas = np.degrees(np.arctan2(y2s - y1s, x2s - x1s) + np.pi)\n",
    "                rhos = np.sqrt((y2s - y1s) ** 2 + (x2s - x1s) ** 2)\n",
    "                rhos_log_space = np.log10(rhos)\n",
    "                \n",
    "                quantized_rhos = np.zeros(rhos.shape, dtype=int)\n",
    "                for i in range(N_RHO_BINS):\n",
    "                    quantized_rhos += (rhos_log_space < rho_bins_edges[i])\n",
    "                    \n",
    "                for i, r_bin in enumerate(quantized_rhos):\n",
    "                    theta_bin = int(thetas[i] // BIN_SIZE) % N_ANGLE_BINS\n",
    "                    hist[r_bin - 1, theta_bin] += 1\n",
    "                \n",
    "            normalised_hist = hist / hist.sum()\n",
    "            feature_vectors[j] = normalised_hist.flatten()\n",
    "            \n",
    "        return feature_vectors.flatten()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2- HINGE features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "#define some constants for hinge \n",
    "N_ANGLE_BINS = 40\n",
    "BIN_SIZE = 360 // N_ANGLE_BINS\n",
    "LEG_LENGTH = 25"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_hinge_features(bw_image):\n",
    "        \n",
    "        contours = get_contour_pixels(bw_image)\n",
    "        \n",
    "        hist = np.zeros((N_ANGLE_BINS, N_ANGLE_BINS))\n",
    "            \n",
    "        # print([len(cnt) for cnt in contours])\n",
    "        for cnt in contours:\n",
    "            n_pixels = len(cnt)\n",
    "            if n_pixels <= LEG_LENGTH:\n",
    "                continue\n",
    "            \n",
    "            points = np.array([point[0] for point in cnt])\n",
    "            xs, ys = points[:, 0], points[:, 1]\n",
    "            point_1s = np.array([cnt[(i + LEG_LENGTH) % n_pixels][0] for i in range(n_pixels)])\n",
    "            point_2s = np.array([cnt[(i - LEG_LENGTH) % n_pixels][0] for i in range(n_pixels)])\n",
    "            x1s, y1s = point_1s[:, 0], point_1s[:, 1]\n",
    "            x2s, y2s = point_2s[:, 0], point_2s[:, 1]\n",
    "            \n",
    "            phi_1s = np.degrees(np.arctan2(y1s - ys, x1s - xs) + np.pi)\n",
    "            phi_2s = np.degrees(np.arctan2(y2s - ys, x2s - xs) + np.pi)\n",
    "            \n",
    "            indices = np.where(phi_2s > phi_1s)[0]\n",
    "            \n",
    "            for i in indices:\n",
    "                phi1 = int(phi_1s[i] // BIN_SIZE) % N_ANGLE_BINS\n",
    "                phi2 = int(phi_2s[i] // BIN_SIZE) % N_ANGLE_BINS\n",
    "                hist[phi1, phi2] += 1\n",
    "                \n",
    "        normalised_hist = hist / np.sum(hist)\n",
    "        feature_vector = normalised_hist[np.triu_indices_from(normalised_hist, k = 1)]\n",
    "        \n",
    "        return feature_vector"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "3- LBP Features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_LBP_features (img) :\n",
    "    radius = 2\n",
    "    n_points = 16 * radius\n",
    "    lbp = local_binary_pattern(img, n_points, radius, method='nri_uniform')\n",
    "    n_bins = n_points * (n_points - 1) + 3\n",
    "    lbp_hist = np.histogram(lbp.ravel(), bins=np.arange(n_bins + 1), density=True)[0]\n",
    "    return lbp_hist"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "4- GLCM Features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_all_glcm_features(gray_scale_img):\n",
    "    \n",
    "    #size of co-occ matrix = number of gray levels\n",
    "    image_array = np.array(gray_scale_img)\n",
    "    #print('first pixel= ', image_array[0][0])\n",
    "    coocurrence_matrix = greycomatrix(image_array, [1], [0, np.pi / 4, np.pi / 2, 3 * np.pi / 4])\n",
    "    contrast = greycoprops(coocurrence_matrix, 'contrast')\n",
    "    homogeneity = greycoprops(coocurrence_matrix, 'homogeneity')\n",
    "    #mean = greycoprops(coocurrence_matrix, 'mean')\n",
    "    energy = greycoprops(coocurrence_matrix, 'energy')\n",
    "    #entropy = greycoprops(coocurrence_matrix, 'entropy')\n",
    "    #variance = greycoprops(coocurrence_matrix, 'variance')\n",
    "    correlation = greycoprops(coocurrence_matrix, 'correlation')\n",
    "    return contrast, homogeneity, energy, correlation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "a function that return all the required features of a given image\n",
    "'''\n",
    "def get_features(img):\n",
    "    '''\n",
    "        glcm_features = []\n",
    "        contrast, homogeneity, energy, correlation= get_all_glcm_features((img * 255).astype(np.uint8))\n",
    "        features = []\n",
    "        features.append(contrast.ravel())\n",
    "        features.append(homogeneity.ravel())\n",
    "        features.append(energy.ravel())\n",
    "        features.append(correlation.ravel())\n",
    "        features = np.array(features).ravel()\n",
    "        glcm_features.append(features)\n",
    "        glcm_features = (np.array(glcm_features)).ravel()\n",
    "    '''    \n",
    "    hinge_features = get_hinge_features(img)\n",
    "    return hinge_features"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Model Selection and Training Module:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "Model Training:\n",
    "    a function that creates an svm model and train it with the given features\n",
    "    then it saves the model after training\n",
    "'''\n",
    "def svm_train(**kwargs):\n",
    "    #first we read the train data images and extract therir features\n",
    "    features = []\n",
    "    for file in glob.glob(Male_training_data):    \n",
    "        img = cv2.imread(file)  #read male images\n",
    "        img = Preprocessing(img)\n",
    "        features.append(np.append(get_features(img),1))\n",
    "\n",
    "    for file in glob.glob(Female_training_data):    \n",
    "        img = cv2.imread(file)  #read female images\n",
    "        img = Preprocessing(img)\n",
    "        features.append(np.append(get_features(img),0))\n",
    "  \n",
    "\n",
    "    features = np.array(features)\n",
    "    x_train = features[:,:-1]\n",
    "    #standered scaler: scales the data to have mean = 0 and standered deviation = 1\n",
    "    std_scaler = StandardScaler()\n",
    "    x_train = std_scaler.fit_transform(x_train)\n",
    "    y_train = features[:,-1]\n",
    "    #svm clasifier\n",
    "    svm_clf = svm.SVC(**kwargs)\n",
    "    svm_clf.fit(x_train, y_train)\n",
    "    # save the model to disk\n",
    "    pickle.dump(svm_clf, open('svm_model.sav', 'wb'))\n",
    "    pickle.dump(std_scaler, open('std_scaler.bin', 'wb'))\n",
    "    return "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Performance Analysis Module:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "function that test a given image with a given svm model and returns it's prediction \n",
    "valu:\n",
    "1 --> male\n",
    "0 --> females\n",
    "'''\n",
    "def svm_test(svm_clf,std_scaler, img):\n",
    "    img = Preprocessing(img)\n",
    "    features = get_features(img)\n",
    "    features = std_scaler.transform([features])\n",
    "    return  svm_clf.predict(features)[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "this functon takes an svm_clf and generates the required reports for it based on the << imaged in the test file >>\n",
    "'''\n",
    "def analyse_performance(svm_clf,std_scaler):\n",
    "    #open 2-files to write the output in them\n",
    "    results_file = open_file(\"results.txt\")\n",
    "    time_file = open_file(\"times.txt\")\n",
    "    m_correct, m_count, f_correct, f_count = 0,0,0,0\n",
    "    #testing all test images\n",
    "    for file in sorted(glob.glob(Male_testing_data)):    \n",
    "        img = cv2.imread(file)  #read male images\n",
    "        time,value = function_timer(svm_test, svm_clf, std_scaler, img)\n",
    "        write_in_file(time_file,time)\n",
    "        write_in_file(results_file,int(value))\n",
    "        m_count+=1\n",
    "        if value == 1:\n",
    "            m_correct+=1\n",
    "\n",
    "    for file in sorted(glob.glob(Female_testing_data)):    \n",
    "        img = cv2.imread(file)  #read female images\n",
    "        time,value = function_timer(svm_test, svm_clf, std_scaler, img)\n",
    "        write_in_file(time_file,time)\n",
    "        write_in_file(results_file,int(value))\n",
    "        f_count+=1\n",
    "        if value == 0:\n",
    "            f_correct+=1\n",
    "\n",
    "    close_file(results_file)\n",
    "    close_file(time_file)\n",
    "\n",
    "    return  (m_correct/m_count)*100, (f_correct/f_count)*100, ((m_correct + f_correct)/(m_count + f_count))*100"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Main blocks:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "#training block\n",
    "svm_train(C=5,gamma='scale',kernel=\"rbf\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "male correct percentage=  95.74468085106383 \n",
      " female correct percentage=  92.5925925925926 \n",
      " total correct percentage=  94.5945945945946\n"
     ]
    }
   ],
   "source": [
    "#testinging block\n",
    "svm_clf = pickle.load(open('svm_model.sav', 'rb'))\n",
    "std_scaler = pickle.load(open('std_scaler.bin', 'rb'))\n",
    "male_percentage, female_percentage, total_percentag = analyse_performance(svm_clf,std_scaler)\n",
    "print(\"male correct percentage= \",male_percentage, \"\\n\", \"female correct percentage= \",female_percentage, \"\\n\", \"total correct percentage= \", total_percentag)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "e0d277ff71215bc2e00e65dbe1083bb095effc9738a74808cbcc90b6ef0d8026"
  },
  "kernelspec": {
   "display_name": "Python 3.8.8 ('base')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
